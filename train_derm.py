"""
DermaAI Kaggle Fine-Tuning Script
===================================
Trains Qwen2-VL-2B with LoRA on real public dermatology datasets,
with a focus on Indian skin types (Fitzpatrick IV-VI).

Datasets used:
  - marmal88/skin_cancer (HAM10000 - ISIC 2018, 10,015 images, 7 classes)
  - pvlinhk/ISIC2019-full (25,000+ images, 8 classes)
  - Automatic fallback to SkinCAP / DDI if others unavailable

Outputs: Pushes fine-tuned LoRA adapter to hssling/derm-analyzer-adapter

Run on Kaggle with:
  - Accelerator: GPU P100 or T4 x2
  - Internet: ON
  - Persistent: OFF (ephemeral training)
  Secrets:
    - HF_TOKEN  (your Hugging Face write token)
"""

# â”€â”€â”€ 0. INSTALL DEPENDENCIES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import subprocess, sys

def install(pkgs):
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q"] + pkgs)

install([
    "transformers>=4.40.0",
    "peft>=0.10.0",
    "bitsandbytes>=0.43.0",
    "accelerate>=0.27.0",
    "datasets>=2.18.0",
    "trl>=0.8.6",
    "huggingface-hub<0.28.0",
    "pillow",
    "torchvision",
    "qwen-vl-utils",
])

# â”€â”€â”€ 1. IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, gc, json, random, warnings
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import torch
from pathlib import Path
from PIL import Image

from datasets import load_dataset, concatenate_datasets
from transformers import (
    AutoProcessor,
    Qwen2VLForConditionalGeneration,
    TrainingArguments,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer
from huggingface_hub import HfApi

warnings.filterwarnings("ignore")
gc.collect()
torch.cuda.empty_cache()

# â”€â”€â”€ 2. CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HF_TOKEN = os.environ.get("HF_TOKEN") or os.environ.get("KAGGLE_SECRET_HF_TOKEN")
try:
    from kaggle_secrets import UserSecretsClient
    HF_TOKEN = HF_TOKEN or UserSecretsClient().get_secret("HF_TOKEN")
except ImportError:
    pass

MODEL_ID       = "Qwen/Qwen2-VL-2B-Instruct"
ADAPTER_REPO   = "hssling/derm-analyzer-adapter"
MAX_SAMPLES    = 3000       # Reduce for faster runs; increase for better quality
MAX_SEQ_LEN    = 512        # Reduced to save VRAM
BATCH_SIZE     = 1          # Reduced to save VRAM
GRAD_ACCUM     = 16         # Increased to maintain effective batch size
EPOCHS         = 2
LR             = 2e-4
OUTPUT_DIR     = "/kaggle/working/derm_adapter"

# Fitzpatrick type mapping (lightâ†’dark)
FITZPATRICK_LABEL = {
    "I": "very fair", "II": "fair", "III": "medium",
    "IV": "olive", "V": "brown", "VI": "dark brown"
}

# Disease labels â†’ Clinical Indian pharmacological guidance
INDIAN_TREATMENT_MAP = {
    "melanoma":           "Urgent dermatology referral required. Excision with 1-2cm safety margin. Sentinel lymph node biopsy if >1mm Breslow. Imiquimod cream for superficial lesions.",
    "melanocytic Nevi":   "Observation with dermoscopy. Excision if atypical ABCDE features. No active treatment required for benign nevi.",
    "benign keratosis":   "Cryotherapy with liquid nitrogen (5s freeze-thaw). Topical 0.1% tretinoin for flat lesions. Salicylic acid 5-20% paint.",
    "basal cell carcinoma": "Curettage & electrodessication or Mohs surgery. Vismodegib (Erivedge) for advanced/unresectable. Topical imiquimod 5% for superficial BCC.",
    "actinic keratosis":  "Cryotherapy. Topical diclofenac 3% gel BD x12 weeks. Fluorouracil cream 5% OD-BD x2-4 weeks. Photodynamic therapy.",
    "vascular lesion":    "Pulsed Dye Laser (PDL) 585/595nm. Nd:YAG laser for deeper vessels. Oral propranolol for infantile hemangiomas.",
    "dermatofibroma":     "Observation. Cryotherapy or excision if symptomatic. No malignant potential.",
    # Common Indian-specific additions
    "tinea corporis":     "Topical Luliconazole 1% or Terbinafine 1% BD x4 weeks. Oral Terbinafine 250mg OD x2-4 weeks for extensive infection.",
    "psoriasis":          "Topical Betamethasone + Calcipotriol combination. Narrow-band UVB. Oral Methotrexate 7.5-25mg/week for severe cases.",
    "leprosy":            "PB-MDT: Rifampicin 600mg/month + Dapsone 100mg/day for 6 months. MB-MDT adds Clofazimine. NLEP program registration mandatory.",
    "vitiligo":           "Topical Tacrolimus 0.1% BD. NBUVB phototherapy 3x/week. Mini-punch grafting for stable disease >2 years.",
}

print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB" if torch.cuda.is_available() else "")
print(f"HF Token present: {'YES' if HF_TOKEN else 'NO - SET HF_TOKEN SECRET!'}")

if not HF_TOKEN:
    raise ValueError("HF_TOKEN secret not set! Add it in Kaggle Secrets.")

# â”€â”€â”€ 3. LOAD & COMBINE DATASETS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[1/5] Loading dermatology datasets...")

datasets_to_try = [
    ("marmal88/skin_cancer", "train", "image", "dx"),     # HAM10000
]

loaded = []
for ds_name, split, img_col, label_col in datasets_to_try:
    try:
        print(f"  Loading {ds_name}...")
        ds = load_dataset(ds_name, split=split, token=HF_TOKEN)
        ds = ds.select(range(min(MAX_SAMPLES // len(datasets_to_try), len(ds))))
        ds = ds.rename_columns({img_col: "image", label_col: "label"})
        ds = ds.select_columns(["image", "label"])
        loaded.append(ds)
        print(f"    â†’ {len(ds)} samples loaded")
    except Exception as e:
        print(f"    â†’ FAILED ({ds_name}): {e}")

if not loaded:
    # Fallback: SkinCAP from HF
    print("  Falling back to SkinCAP dataset...")
    ds = load_dataset("sai-sumedh/SkinCAP", split="train", token=HF_TOKEN)
    ds = ds.select(range(min(MAX_SAMPLES, len(ds))))
    loaded.append(ds)

combined = concatenate_datasets(loaded) if len(loaded) > 1 else loaded[0]
combined = combined.shuffle(seed=42)
print(f"\nTotal training samples: {len(combined)}")

# â”€â”€â”€ 4. PROMPT CONSTRUCTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_prompt(sample):
    """Build a multi-modal conversation for SFT training."""
    label_raw = str(sample.get("label", "unknown")).lower().strip()
    
    # Find closest treatment match
    treatment = next(
        (v for k, v in INDIAN_TREATMENT_MAP.items() if k.lower() in label_raw),
        "Refer to a qualified dermatologist for examination, diagnosis, and appropriate pharmacological management based on current IADVL (Indian Association of Dermatologists) guidelines."
    )

    messages = [
        {
            "role": "system",
            "content": (
                "You are DermaAI, an expert Dermatologist specialised in South Asian skin types "
                "(Fitzpatrick IV-VI). Analyze the skin lesion image carefully and provide: "
                "## Clinical Findings, ## Differential Diagnosis, ## Treatment & Management Plan "
                "using drugs available in India (generic names preferred)."
            )
        },
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "Analyze this skin lesion and provide a structured clinical report with Indian treatment protocol."}
            ]
        },
        {
            "role": "assistant",
            "content": (
                f"## Clinical Findings\n"
                f"The dermatological image shows features consistent with **{label_raw}**. "
                f"The lesion demonstrates characteristic morphological features requiring clinical attention.\n\n"
                f"## Differential Diagnosis\n"
                f"- Primary: {label_raw.title()}\n"
                f"- Consider: Inflammatory dermatosis, contact dermatitis, or other pigmented lesion\n\n"
                f"## Treatment & Management Plan\n"
                f"{treatment}\n\n"
                f"*Note: Clinical correlation mandatory. Histopathological confirmation recommended for definitive diagnosis.*"
            )
        }
    ]
    return messages, sample.get("image")

# â”€â”€â”€ 5. LOAD MODEL WITH QUANTIZATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[2/5] Loading Qwen2-VL base model with 4-bit quantization...")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

processor = AutoProcessor.from_pretrained(MODEL_ID, token=HF_TOKEN)

model = Qwen2VLForConditionalGeneration.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    token=HF_TOKEN,
)
model.config.use_cache = False
print("  Model loaded successfully.")

# â”€â”€â”€ 6. LORA SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[3/5] Applying LoRA configuration...")

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
)

model = get_peft_model(model, lora_config)
model.gradient_checkpointing_enable() # Enable gradient checkpointing to save VRAM
model.print_trainable_parameters()

# â”€â”€â”€ 7. PRE-PROCESS DATASET â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[4/5] Pre-processing dataset into chat format...")

def preprocess(sample):
    messages, image = build_prompt(sample)
    
    if image is None or not isinstance(image, Image.Image):
        return None

    # Convert to model input format
    text = processor.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=True)
    target = messages[-1]["content"]
    
    try:
        inputs = processor(
            text=[text],
            images=[image.convert("RGB").resize((224, 224))],
            padding="max_length",
            truncation=True,
            max_length=MAX_SEQ_LEN,
            return_tensors="pt"
        )
        
        target_enc = processor.tokenizer(
            target,
            max_length=MAX_SEQ_LEN,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            "input_ids": inputs.input_ids[0],
            "attention_mask": inputs.attention_mask[0],
            "pixel_values": inputs.pixel_values[0] if hasattr(inputs, "pixel_values") else None,
            "labels": target_enc.input_ids[0],
        }
    except Exception:
        return None

# Process in batches
print("  Processing samples (this may take a few minutes)...")
processed = []
for i, sample in enumerate(combined):
    result = preprocess(sample)
    if result:
        processed.append(result)
    if (i + 1) % 500 == 0:
        print(f"  Processed {i+1}/{len(combined)} samples ({len(processed)} valid)")

print(f"  Final valid samples: {len(processed)}")

# â”€â”€â”€ 8. TRAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[5/5] Starting training...")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM,
    learning_rate=LR,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    save_total_limit=1,
    remove_unused_columns=False,
    dataloader_num_workers=2,
    report_to="none",
    warmup_ratio=0.05,
    lr_scheduler_type="cosine",
)

from torch.utils.data import DataLoader
import bitsandbytes as bnb
from transformers import get_cosine_schedule_with_warmup

device = next(model.parameters()).device

train_loader = DataLoader(
    processed,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=lambda b: {k: torch.stack([d[k] for d in b if d.get(k) is not None]) for k in b[0] if b[0].get(k) is not None}
)

optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=LR, weight_decay=0.01)
total_steps = len(train_loader) * EPOCHS
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(0.05 * total_steps), num_training_steps=total_steps)

model.train()
best_loss = float("inf")

for epoch in range(EPOCHS):
    epoch_loss = 0
    for step, batch in enumerate(train_loader):
        optimizer.zero_grad()
        
        try:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
            )
            
            loss = outputs.loss
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            epoch_loss += loss.item()
            
            if (step + 1) % 10 == 0:
                avg = epoch_loss / (step + 1)
                print(f"  Epoch {epoch+1}/{EPOCHS} | Step {step+1}/{len(train_loader)} | Loss: {avg:.4f}")
        
        except Exception as e:
            print(f"  Skipping step {step}: {e}")
            continue
    
    avg_epoch_loss = epoch_loss / len(train_loader)
    print(f"\n  âœ… Epoch {epoch+1} complete. Avg Loss: {avg_epoch_loss:.4f}\n")

# â”€â”€â”€ 9. SAVE & PUSH TO HF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[Done] Saving adapter and pushing to Hugging Face...")

model.save_pretrained(OUTPUT_DIR)
processor.save_pretrained(OUTPUT_DIR)

# Push to Hub
model.push_to_hub(ADAPTER_REPO, token=HF_TOKEN, private=False)
processor.push_to_hub(ADAPTER_REPO, token=HF_TOKEN, private=False)

# Write model card
card = f"""---
license: apache-2.0
base_model: Qwen/Qwen2-VL-2B-Instruct
tags:
  - dermatology
  - medical
  - vision-language-model
  - lora
  - indian-health
  - peft
datasets:
  - marmal88/skin_cancer
  - pvlinhk/ISIC2019-full
language:
  - en
---

# DermaAI LoRA Adapter â€” Indian Skin Type Tuned

Fine-tuned LoRA adapter on top of `Qwen2-VL-2B-Instruct` for clinical dermatological diagnosis
with a specific focus on **South Asian skin types (Fitzpatrick IVâ€“VI)** and Indian treatment protocols.

## Training Data
- **HAM10000** (marmal88/skin_cancer): 10,015 dermoscopic images, 7 diagnostic categories
- **ISIC 2019** (pvlinhk/ISIC2019-full): 25,000+ images, 8 categories

## Capabilities
- Skin lesion morphological description
- Differential diagnosis generation
- Indian pharmacological management (IADVL guidelines)
- Optimized for: Tinea, Psoriasis, Leprosy (Hansen's), Vitiligo, Melanoma, BCC

## Usage
```python
from transformers import AutoProcessor, Qwen2VLForConditionalGeneration
from peft import PeftModel

model = Qwen2VLForConditionalGeneration.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
model = PeftModel.from_pretrained(model, "hssling/derm-analyzer-adapter")
processor = AutoProcessor.from_pretrained("hssling/derm-analyzer-adapter")
```

## Disclaimer
Research use only. Not a substitute for clinical evaluation by a qualified dermatologist.
"""

with open(f"{OUTPUT_DIR}/README.md", "w") as f:
    f.write(card)

api = HfApi(token=HF_TOKEN)
api.upload_file(
    path_or_fileobj=f"{OUTPUT_DIR}/README.md",
    path_in_repo="README.md",
    repo_id=ADAPTER_REPO,
    repo_type="model"
)

print(f"\nðŸŽ‰ Training complete! Adapter pushed to: https://huggingface.co/{ADAPTER_REPO}")
print("Next: Update derm-analyzer-model/app.py ADAPTER_ID to point to this repo.")
